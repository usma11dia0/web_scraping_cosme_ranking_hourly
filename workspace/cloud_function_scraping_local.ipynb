{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5b3636-7410-4512-8b8f-d5b1c21ec30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install selenium==3.13.0\n",
    "!pip -q install google-cloud-storage\n",
    "!pip -q install gspread\n",
    "!pip -q install gspread_dataframe\n",
    "!pip -q install google-auth\n",
    "!pip -q install google-api-python-client\n",
    "!pip -q install python-dotenv\n",
    "!pip -q install pytz\n",
    "# !pip -q install google-cloud-secretmanager 本番環境のみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0a0adc-7412-4dd7-a8f5-0a6c35274a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 00:06:32,084 [INFO] main: ------スクレイピングを開始します------\n",
      "2023-04-10 00:06:32,866 [INFO] main: chrome-headless.zipの解凍に成功しました\n",
      "2023-04-10 00:06:32,868 [DEBUG] main: ChromeDriverのパス: /workspace/chrome-headless/chromedriver\n",
      "2023-04-10 00:06:32,868 [DEBUG] main: ChromeDriverのパス: /workspace/chrome-headless/headless-chromium\n",
      "2023-04-10 00:06:32,876 [INFO] main: chrome-headless配下フォルダに実行権限を付与しました\n",
      "2023-04-10 00:06:34,025 [INFO] main: ChromeDriverを起動しました\n",
      "2023-04-10 00:06:34,027 [INFO] main: ドライバ準備完了\n",
      "2023-04-10 00:06:34,028 [INFO] main: ------楽天取得開始------\n",
      "2023-04-10 00:06:35,472 [DEBUG] main: 楽天：トップページへ移動しました\n",
      "2023-04-10 00:06:35,473 [DEBUG] main: ------1ページ目------\n",
      "2023-04-10 00:06:35,592 [DEBUG] main: 楽天：ページリンクを取得しました\n",
      "2023-04-10 00:06:36,075 [DEBUG] main: 楽天：1～3位までの順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:06:45,082 [DEBUG] main: 楽天：4位以降の順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:06:46,014 [DEBUG] main: 楽天：2ページ目へ移動しました\n",
      "2023-04-10 00:06:46,015 [DEBUG] main: ------2ページ目------\n",
      "2023-04-10 00:06:48,228 [DEBUG] main: 楽天：4位以降の順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:06:48,229 [DEBUG] main: ------取得結果------\n",
      "2023-04-10 00:06:48,230 [DEBUG] main: 楽天：\"楽天\" 100個\n",
      "2023-04-10 00:06:48,231 [DEBUG] main: 楽天：\"商品名\" 100個\n",
      "2023-04-10 00:06:48,231 [DEBUG] main: 楽天：\"会社名\" 100個\n",
      "2023-04-10 00:06:48,232 [DEBUG] main: 楽天：\"価格\" 100個\n",
      "2023-04-10 00:06:48,233 [DEBUG] main: 楽天：\"URL\" 100個\n",
      "2023-04-10 00:06:48,233 [INFO] main: ------楽天取得完了------\n",
      "2023-04-10 00:06:48,234 [INFO] main: ------AMAZON取得開始------\n",
      "2023-04-10 00:06:49,704 [DEBUG] main: Amazon：トップページへ移動しました\n",
      "2023-04-10 00:06:49,705 [DEBUG] main: ------1ページ目------\n",
      "2023-04-10 00:07:03,448 [DEBUG] main: Amazon：ページリンクを取得しました\n",
      "2023-04-10 00:07:17,860 [DEBUG] main: Amazon：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:07:19,043 [DEBUG] main: Amazon：2ページ目へ移動しました\n",
      "2023-04-10 00:07:19,044 [DEBUG] main: ------2ページ目------\n",
      "2023-04-10 00:07:45,258 [DEBUG] main: Amazon：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:07:45,259 [DEBUG] main: ------取得結果------\n",
      "2023-04-10 00:07:45,260 [DEBUG] main: Amazon：\"Amazon\" 100個\n",
      "2023-04-10 00:07:45,260 [DEBUG] main: Amazon：\"商品名\" 100個\n",
      "2023-04-10 00:07:45,261 [DEBUG] main: Amazon：\"会社名\" 100個\n",
      "2023-04-10 00:07:45,261 [DEBUG] main: Amazon：\"価格\" 100個\n",
      "2023-04-10 00:07:45,262 [DEBUG] main: Amazon：\"URL\" 100個\n",
      "2023-04-10 00:07:45,262 [INFO] main: ------AMAZON取得完了------\n",
      "2023-04-10 00:07:45,263 [INFO] main: ------Yahoo取得開始------\n",
      "2023-04-10 00:07:47,032 [DEBUG] main: ヤフーショッピング：トップページへ移動しました\n",
      "2023-04-10 00:07:47,033 [DEBUG] main: ------1ページ目------\n",
      "2023-04-10 00:07:47,083 [DEBUG] main: ヤフーショッピング：ページリンクを取得しました\n",
      "2023-04-10 00:07:49,137 [DEBUG] main: ヤフーショッピング：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:07:50,639 [DEBUG] main: ヤフーショッピング：2ページ目へ移動しました\n",
      "2023-04-10 00:07:50,640 [DEBUG] main: ------2ページ目------\n",
      "2023-04-10 00:07:52,667 [DEBUG] main: ヤフーショッピング：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:07:54,154 [DEBUG] main: ヤフーショッピング：3ページ目へ移動しました\n",
      "2023-04-10 00:07:54,155 [DEBUG] main: ------3ページ目------\n",
      "2023-04-10 00:07:56,181 [DEBUG] main: ヤフーショッピング：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:07:57,483 [DEBUG] main: ヤフーショッピング：4ページ目へ移動しました\n",
      "2023-04-10 00:07:57,485 [DEBUG] main: ------4ページ目------\n",
      "2023-04-10 00:07:59,561 [DEBUG] main: ヤフーショッピング：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:08:00,900 [DEBUG] main: ヤフーショッピング：5ページ目へ移動しました\n",
      "2023-04-10 00:08:00,901 [DEBUG] main: ------5ページ目------\n",
      "2023-04-10 00:08:02,968 [DEBUG] main: ヤフーショッピング：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:08:02,969 [DEBUG] main: ------取得結果------\n",
      "2023-04-10 00:08:02,969 [DEBUG] main: ヤフーショッピング：\"ヤフーショッピング\" 100個\n",
      "2023-04-10 00:08:02,970 [DEBUG] main: ヤフーショッピング：\"商品名\" 100個\n",
      "2023-04-10 00:08:02,971 [DEBUG] main: ヤフーショッピング：\"会社名\" 100個\n",
      "2023-04-10 00:08:02,971 [DEBUG] main: ヤフーショッピング：\"価格\" 100個\n",
      "2023-04-10 00:08:02,972 [DEBUG] main: ヤフーショッピング：\"URL\" 100個\n",
      "2023-04-10 00:08:02,977 [INFO] main: ------Yahoo取得完了------\n",
      "2023-04-10 00:08:02,978 [INFO] main: ------QOO10取得開始------\n",
      "2023-04-10 00:08:14,273 [DEBUG] main: Qoo10：トップページへ移動しました\n",
      "2023-04-10 00:08:14,274 [DEBUG] main: ------1ページ目------\n",
      "2023-04-10 00:08:55,540 [DEBUG] main: Qoo10：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:08:55,541 [DEBUG] main: ------取得結果------\n",
      "2023-04-10 00:08:55,542 [DEBUG] main: Qoo10：\"Qoo10\" 100個\n",
      "2023-04-10 00:08:55,542 [DEBUG] main: Qoo10：\"商品名\" 100個\n",
      "2023-04-10 00:08:55,543 [DEBUG] main: Qoo10：\"会社名\" 100個\n",
      "2023-04-10 00:08:55,544 [DEBUG] main: Qoo10：\"価格\" 100個\n",
      "2023-04-10 00:08:55,544 [DEBUG] main: Qoo10：\"URL\" 100個\n",
      "2023-04-10 00:08:55,545 [INFO] main: ------QOO10取得完了------\n",
      "2023-04-10 00:08:55,545 [INFO] main: ------アットコスメ取得開始------\n",
      "2023-04-10 00:09:00,530 [DEBUG] main: アットコスメ：トップページへ移動しました\n",
      "2023-04-10 00:09:00,531 [DEBUG] main: ------1ページ目------\n",
      "2023-04-10 00:09:00,605 [DEBUG] main: アットコスメ：ページリンクを取得しました\n",
      "2023-04-10 00:09:01,679 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:05,684 [DEBUG] main: アットコスメ：2ページ目へ移動しました\n",
      "2023-04-10 00:09:05,685 [DEBUG] main: ------2ページ目------\n",
      "2023-04-10 00:09:06,828 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:09,341 [DEBUG] main: アットコスメ：3ページ目へ移動しました\n",
      "2023-04-10 00:09:09,342 [DEBUG] main: ------3ページ目------\n",
      "2023-04-10 00:09:10,475 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:14,496 [DEBUG] main: アットコスメ：4ページ目へ移動しました\n",
      "2023-04-10 00:09:14,497 [DEBUG] main: ------4ページ目------\n",
      "2023-04-10 00:09:15,765 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:19,502 [DEBUG] main: アットコスメ：5ページ目へ移動しました\n",
      "2023-04-10 00:09:19,504 [DEBUG] main: ------5ページ目------\n",
      "2023-04-10 00:09:20,635 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:23,712 [DEBUG] main: アットコスメ：6ページ目へ移動しました\n",
      "2023-04-10 00:09:23,713 [DEBUG] main: ------6ページ目------\n",
      "2023-04-10 00:09:24,808 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:29,297 [DEBUG] main: アットコスメ：7ページ目へ移動しました\n",
      "2023-04-10 00:09:29,298 [DEBUG] main: ------7ページ目------\n",
      "2023-04-10 00:09:30,473 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:33,310 [DEBUG] main: アットコスメ：8ページ目へ移動しました\n",
      "2023-04-10 00:09:33,314 [DEBUG] main: ------8ページ目------\n",
      "2023-04-10 00:09:34,450 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:37,171 [DEBUG] main: アットコスメ：9ページ目へ移動しました\n",
      "2023-04-10 00:09:37,173 [DEBUG] main: ------9ページ目------\n",
      "2023-04-10 00:09:38,411 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:40,790 [DEBUG] main: アットコスメ：10ページ目へ移動しました\n",
      "2023-04-10 00:09:40,792 [DEBUG] main: ------10ページ目------\n",
      "2023-04-10 00:09:41,920 [DEBUG] main: アットコスメ：順位/商品名/会社名/価格/URLを取得しました\n",
      "2023-04-10 00:09:41,920 [DEBUG] main: ------取得結果------\n",
      "2023-04-10 00:09:41,921 [DEBUG] main: アットコスメ：\"アットコスメ\" 100個\n",
      "2023-04-10 00:09:41,922 [DEBUG] main: アットコスメ：\"商品名\" 100個\n",
      "2023-04-10 00:09:41,922 [DEBUG] main: アットコスメ：\"会社名\" 100個\n",
      "2023-04-10 00:09:41,923 [DEBUG] main: アットコスメ：\"価格\" 100個\n",
      "2023-04-10 00:09:41,924 [DEBUG] main: アットコスメ：\"URL\" 100個\n",
      "2023-04-10 00:09:41,926 [INFO] main: ------アットコスメ取得完了------\n",
      "2023-04-10 00:09:41,926 [INFO] main: ------レサージュ取得開始------\n",
      "2023-04-10 00:09:44,465 [DEBUG] main: レサージュ：トップページへ移動しました\n",
      "2023-04-10 00:09:44,466 [DEBUG] main: ------1ページ目------\n",
      "2023-04-10 00:09:44,494 [DEBUG] main: レサージュ：ページリンクを取得しました\n",
      "2023-04-10 00:09:47,792 [DEBUG] main: レサージュ：順位/商品名/会社名/価格/URLを取得しました。\n",
      "2023-04-10 00:09:49,405 [DEBUG] main: レサージュ：2ページ目へ移動しました\n",
      "2023-04-10 00:09:49,406 [DEBUG] main: ------2ページ目------\n",
      "2023-04-10 00:09:51,133 [DEBUG] main: レサージュ：順位/商品名/会社名/価格/URLを取得しました。\n",
      "2023-04-10 00:09:51,133 [DEBUG] main: ------取得結果------\n",
      "2023-04-10 00:09:51,134 [DEBUG] main: レサージュ：\"レサージュ\" 100個\n",
      "2023-04-10 00:09:51,135 [DEBUG] main: レサージュ：\"商品名\" 100個\n",
      "2023-04-10 00:09:51,136 [DEBUG] main: レサージュ：\"会社名\" 100個\n",
      "2023-04-10 00:09:51,137 [DEBUG] main: レサージュ：\"価格\" 100個\n",
      "2023-04-10 00:09:51,137 [DEBUG] main: レサージュ：\"URL\" 100個\n",
      "2023-04-10 00:09:51,138 [INFO] main: ------レサージュ取得完了------\n",
      "2023-04-10 00:09:51,927 [INFO] main: データ取得フォルダのファイル一覧を取得しました\n",
      "2023-04-10 00:09:54,685 [INFO] main: シート 9時取得 を新規作成しました。\n",
      "2023-04-10 00:09:55,516 [INFO] main: GoogleSpreadSheetへ転記しました\n",
      "2023-04-10 00:09:55,519 [INFO] main: GoogleSpreadSheetへの転記が完了しました\n",
      "2023-04-10 00:09:55,520 [INFO] main: ------スクレイピングを終了します------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>楽天</th>\n",
       "      <th>商品名</th>\n",
       "      <th>会社名</th>\n",
       "      <th>価格</th>\n",
       "      <th>URL</th>\n",
       "      <th>Amazon</th>\n",
       "      <th>商品名</th>\n",
       "      <th>会社名</th>\n",
       "      <th>価格</th>\n",
       "      <th>URL</th>\n",
       "      <th>...</th>\n",
       "      <th>アットコスメ</th>\n",
       "      <th>商品名</th>\n",
       "      <th>会社名</th>\n",
       "      <th>価格</th>\n",
       "      <th>URL</th>\n",
       "      <th>レサージュ</th>\n",
       "      <th>商品名</th>\n",
       "      <th>会社名</th>\n",
       "      <th>価格</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>【クーポンで679円】シートマスク プラセンタエキス等50%配合 30枚入り ホワイト オー...</td>\n",
       "      <td>クオリティファースト　楽天市場店</td>\n",
       "      <td>2,508</td>\n",
       "      <td>https://item.rakuten.co.jp/q1st/10000002/?l2-i...</td>\n",
       "      <td>1</td>\n",
       "      <td>VTCOSMETICS(ブイティコスメテックス) 【正規品】シカデイリースージングマスク フ...</td>\n",
       "      <td>VTCOSMETICS</td>\n",
       "      <td>2,420</td>\n",
       "      <td>https://www.amazon.co.jp/VTCOSMETICS-%E3%83%96...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>リップモンスター</td>\n",
       "      <td>ケイト</td>\n",
       "      <td>880</td>\n",
       "      <td>https://www.cosme.net/product/product_id/10206...</td>\n",
       "      <td>1</td>\n",
       "      <td>ロート製薬 DRX 保湿乳液 ADパーフェクトバリア ボディミルク 130ml</td>\n",
       "      <td>ロート製薬</td>\n",
       "      <td>1,840</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>【50％～51%OFFクーポン】680円(2点購入での1点あたり) 半額 パック シートマス...</td>\n",
       "      <td>プリュ公式ショップ 楽天市場店</td>\n",
       "      <td>1,390</td>\n",
       "      <td>https://item.rakuten.co.jp/luire/pmm-yami/?l2-...</td>\n",
       "      <td>2</td>\n",
       "      <td>イニスフリー (innisfree) ノーセバム ミネラルパウダー N 5グラム (x 1)</td>\n",
       "      <td>イニスフリー</td>\n",
       "      <td>825</td>\n",
       "      <td>https://www.amazon.co.jp/%E3%82%A4%E3%83%8B%E3...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>リポソーム アドバンスト リペアセラム</td>\n",
       "      <td>コスメデコルテ</td>\n",
       "      <td>8,250</td>\n",
       "      <td>https://www.cosme.net/product/product_id/10209...</td>\n",
       "      <td>2</td>\n",
       "      <td>ロート製薬 DRX ハイドロキノンクリーム HQダブルブライトE</td>\n",
       "      <td>ロート製薬</td>\n",
       "      <td>2,000</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>【アテニア 公式】スキンクリア クレンズ オイル エコパック(全2種) 送料無料[Atten...</td>\n",
       "      <td>アテニア公式ショップ　楽天市場店</td>\n",
       "      <td>3,300</td>\n",
       "      <td>https://item.rakuten.co.jp/attenir/166013/?l2-...</td>\n",
       "      <td>3</td>\n",
       "      <td>メラノCC ディープクリア酵素洗顔 130g 酵素×ビタミンC配合 洗顔フォーム 毛穴ケア</td>\n",
       "      <td>メラノCC</td>\n",
       "      <td>715</td>\n",
       "      <td>https://www.amazon.co.jp/%E3%80%90%E5%85%88%E8...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>リポソーム アドバンスト リペアクリーム</td>\n",
       "      <td>コスメデコルテ</td>\n",
       "      <td>11,000</td>\n",
       "      <td>https://www.cosme.net/product/product_id/10225...</td>\n",
       "      <td>3</td>\n",
       "      <td>ロート製薬 DRX 保湿乳液 ADパーフェクトバリア フェイスミルク 50ml</td>\n",
       "      <td>ロート製薬</td>\n",
       "      <td>1,840</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[エントリー最大100%ポイントバック+エントリーP14倍10日9:59マデ]Yunth(ユ...</td>\n",
       "      <td>Yunth Online Store 楽天市場店</td>\n",
       "      <td>3,960</td>\n",
       "      <td>https://item.rakuten.co.jp/yunth/10000000/?l2-...</td>\n",
       "      <td>4</td>\n",
       "      <td>アテニア (Attenir) スキンクリア クレンズオイル アロマタイプ [ レギュラーボト...</td>\n",
       "      <td>アテニア</td>\n",
       "      <td>1,870</td>\n",
       "      <td>https://www.amazon.co.jp/Attenir-%E3%82%B9%E3%...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>ディオール アディクト リップ マキシマイザー</td>\n",
       "      <td>ディオール</td>\n",
       "      <td>4,620</td>\n",
       "      <td>https://www.cosme.net/product/product_id/10233...</td>\n",
       "      <td>4</td>\n",
       "      <td>ZO SKIN HEALTH ゼオスキンヘルス バランサートナー</td>\n",
       "      <td>ゼオスキンヘルス</td>\n",
       "      <td>6,400</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>【アテニア 公式】スキンクリア クレンズ オイル (レギュラーボトル)(全2種) 送料無料[...</td>\n",
       "      <td>アテニア公式ショップ　楽天市場店</td>\n",
       "      <td>1,870</td>\n",
       "      <td>https://item.rakuten.co.jp/attenir/166011/?l2-...</td>\n",
       "      <td>5</td>\n",
       "      <td>ビオレUＶ アクアリッチ アクアプロテクトミスト 60ミリリットル (x 1)</td>\n",
       "      <td>ビオレUＶ</td>\n",
       "      <td>891</td>\n",
       "      <td>https://www.amazon.co.jp/%E3%83%93%E3%82%AA%E3...</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>クリーミータッチライナー</td>\n",
       "      <td>キャンメイク</td>\n",
       "      <td>715</td>\n",
       "      <td>https://www.cosme.net/product/product_id/10147...</td>\n",
       "      <td>5</td>\n",
       "      <td>NEW プラスリストアシリーズ plus RESTORE UVローション（日焼け止め）SPF...</td>\n",
       "      <td>プラスリストア</td>\n",
       "      <td>2,800</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>★ポイント10倍＆割引クーポン★コスメデコルテ リポソーム アドバンスト リペアセラム 10...</td>\n",
       "      <td>コスメ　ヴィーナス　楽天市場店</td>\n",
       "      <td>12,870</td>\n",
       "      <td>https://item.rakuten.co.jp/cosme-venus/4971710...</td>\n",
       "      <td>96</td>\n",
       "      <td>ビオレ ザフェイス ディープモイスト つめかえ用 340ml(約2.1回分)【泡洗顔】【まさ...</td>\n",
       "      <td>ビオレ</td>\n",
       "      <td>1,078</td>\n",
       "      <td>https://www.amazon.co.jp/%E3%83%93%E3%82%AA%E3...</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>ナチュラル チークN</td>\n",
       "      <td>セザンヌ</td>\n",
       "      <td>396</td>\n",
       "      <td>https://www.cosme.net/product/product_id/27441...</td>\n",
       "      <td>96</td>\n",
       "      <td>Revision Skincare リビジョン スキンケア D.E.J face cream...</td>\n",
       "      <td>リビジョン スキンケア</td>\n",
       "      <td>&lt;selenium.webdriver.remote.webelement.WebEleme...</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>【レビューNo.1】【国内正規品】ザ プロダクト ヘアワックス 42g 1個 product...</td>\n",
       "      <td>ビューファ</td>\n",
       "      <td>1,503</td>\n",
       "      <td>https://item.rakuten.co.jp/mobility2/r180503-01n/</td>\n",
       "      <td>97</td>\n",
       "      <td>【先行販売】ビオレ ザクレンズ オイルメイク落とし 本体 190ml クレンジングオイル ク...</td>\n",
       "      <td>ビオレ</td>\n",
       "      <td>1,192</td>\n",
       "      <td>https://www.amazon.co.jp/%E3%80%90%E5%85%88%E8...</td>\n",
       "      <td>...</td>\n",
       "      <td>97</td>\n",
       "      <td>パドル ブラシ</td>\n",
       "      <td>AVEDA(アヴェダ)</td>\n",
       "      <td>3,740</td>\n",
       "      <td>https://www.cosme.net/product/product_id/34756...</td>\n",
       "      <td>97</td>\n",
       "      <td>［デオドラント シリーズ］D-bar（ディーバー）制汗スティック</td>\n",
       "      <td>ディーバー</td>\n",
       "      <td>&lt;selenium.webdriver.remote.webelement.WebEleme...</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>【アテニア 公式】スキンクリア クレンズ オイル エコパック＋ポンプセット(全2種) 送料無...</td>\n",
       "      <td>アテニア公式ショップ　楽天市場店</td>\n",
       "      <td>3,352</td>\n",
       "      <td>https://item.rakuten.co.jp/attenir/166103/</td>\n",
       "      <td>98</td>\n",
       "      <td>大洋 製薬 ワセリンHG クリーム 単品 100グラム (x 1)</td>\n",
       "      <td>大洋</td>\n",
       "      <td>784</td>\n",
       "      <td>https://www.amazon.co.jp/%E5%A4%A7%E6%B4%8B%E8...</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>クリア サンケア スティック</td>\n",
       "      <td>SHISEIDO</td>\n",
       "      <td>3,080</td>\n",
       "      <td>https://www.cosme.net/product/product_id/10206...</td>\n",
       "      <td>98</td>\n",
       "      <td>人気商品 お買い得 ［お買い得セット］HERRAS ミルキーピール エムディーソープ 乳酸菌...</td>\n",
       "      <td>ミルキーピール</td>\n",
       "      <td>&lt;selenium.webdriver.remote.webelement.WebEleme...</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>【国産クレイ配合！ お得な3個セット 送料無料 W洗顔不要 とろけるクレンジング】ink. ...</td>\n",
       "      <td>ink.オンラインストア楽天市場店</td>\n",
       "      <td>2,980</td>\n",
       "      <td>https://item.rakuten.co.jp/129-ink/ink90-3/</td>\n",
       "      <td>99</td>\n",
       "      <td>【医薬部外品】ビオレ マシュマロホイップ 薬用アクネケア つめかえ用 大容量 洗顔 さわやか...</td>\n",
       "      <td>ビオレ</td>\n",
       "      <td>1,089</td>\n",
       "      <td>https://www.amazon.co.jp/%E3%80%90%E5%8C%BB%E8...</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>マイラッシュ アドバンスト</td>\n",
       "      <td>オペラ</td>\n",
       "      <td>999</td>\n",
       "      <td>https://www.cosme.net/product/product_id/10121...</td>\n",
       "      <td>99</td>\n",
       "      <td>資生堂 ナビジョンDR『みずみずしく潤う』セット</td>\n",
       "      <td>資生堂</td>\n",
       "      <td>&lt;selenium.webdriver.remote.webelement.WebEleme...</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>エプソムソルト (3kg)選べる12種類の香り バスソルト 硫酸マグネシウム 入浴剤 マグネ...</td>\n",
       "      <td>リバティライフ楽天市場店</td>\n",
       "      <td>1,980</td>\n",
       "      <td>https://item.rakuten.co.jp/libertylife/201019-2/</td>\n",
       "      <td>100</td>\n",
       "      <td>無印良品 マイルド保湿洗顔フォーム(大容量) 200g 37280724</td>\n",
       "      <td>無印良品</td>\n",
       "      <td>990</td>\n",
       "      <td>https://www.amazon.co.jp/%E7%84%A1%E5%8D%B0%E8...</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>メイベリン スカイハイ</td>\n",
       "      <td>メイベリン ニューヨーク</td>\n",
       "      <td>1,639</td>\n",
       "      <td>https://www.cosme.net/product/product_id/10225...</td>\n",
       "      <td>100</td>\n",
       "      <td>資生堂 ナビジョンDR TAマイルドプロテクトUV</td>\n",
       "      <td>資生堂</td>\n",
       "      <td>&lt;selenium.webdriver.remote.webelement.WebEleme...</td>\n",
       "      <td>https://www.lesage-store.jp/products/detail/431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     楽天                                                商品名  \\\n",
       "0     1  【クーポンで679円】シートマスク プラセンタエキス等50%配合 30枚入り ホワイト オー...   \n",
       "1     2  【50％～51%OFFクーポン】680円(2点購入での1点あたり) 半額 パック シートマス...   \n",
       "2     3  【アテニア 公式】スキンクリア クレンズ オイル エコパック(全2種) 送料無料[Atten...   \n",
       "3     4  [エントリー最大100%ポイントバック+エントリーP14倍10日9:59マデ]Yunth(ユ...   \n",
       "4     5  【アテニア 公式】スキンクリア クレンズ オイル (レギュラーボトル)(全2種) 送料無料[...   \n",
       "..  ...                                                ...   \n",
       "95   96  ★ポイント10倍＆割引クーポン★コスメデコルテ リポソーム アドバンスト リペアセラム 10...   \n",
       "96   97  【レビューNo.1】【国内正規品】ザ プロダクト ヘアワックス 42g 1個 product...   \n",
       "97   98  【アテニア 公式】スキンクリア クレンズ オイル エコパック＋ポンプセット(全2種) 送料無...   \n",
       "98   99  【国産クレイ配合！ お得な3個セット 送料無料 W洗顔不要 とろけるクレンジング】ink. ...   \n",
       "99  100  エプソムソルト (3kg)選べる12種類の香り バスソルト 硫酸マグネシウム 入浴剤 マグネ...   \n",
       "\n",
       "                         会社名      価格  \\\n",
       "0           クオリティファースト　楽天市場店   2,508   \n",
       "1            プリュ公式ショップ 楽天市場店   1,390   \n",
       "2           アテニア公式ショップ　楽天市場店   3,300   \n",
       "3   Yunth Online Store 楽天市場店   3,960   \n",
       "4           アテニア公式ショップ　楽天市場店   1,870   \n",
       "..                       ...     ...   \n",
       "95           コスメ　ヴィーナス　楽天市場店  12,870   \n",
       "96                     ビューファ   1,503   \n",
       "97          アテニア公式ショップ　楽天市場店   3,352   \n",
       "98         ink.オンラインストア楽天市場店   2,980   \n",
       "99              リバティライフ楽天市場店   1,980   \n",
       "\n",
       "                                                  URL Amazon  \\\n",
       "0   https://item.rakuten.co.jp/q1st/10000002/?l2-i...      1   \n",
       "1   https://item.rakuten.co.jp/luire/pmm-yami/?l2-...      2   \n",
       "2   https://item.rakuten.co.jp/attenir/166013/?l2-...      3   \n",
       "3   https://item.rakuten.co.jp/yunth/10000000/?l2-...      4   \n",
       "4   https://item.rakuten.co.jp/attenir/166011/?l2-...      5   \n",
       "..                                                ...    ...   \n",
       "95  https://item.rakuten.co.jp/cosme-venus/4971710...     96   \n",
       "96  https://item.rakuten.co.jp/mobility2/r180503-01n/     97   \n",
       "97         https://item.rakuten.co.jp/attenir/166103/     98   \n",
       "98        https://item.rakuten.co.jp/129-ink/ink90-3/     99   \n",
       "99   https://item.rakuten.co.jp/libertylife/201019-2/    100   \n",
       "\n",
       "                                                  商品名          会社名     価格  \\\n",
       "0   VTCOSMETICS(ブイティコスメテックス) 【正規品】シカデイリースージングマスク フ...  VTCOSMETICS  2,420   \n",
       "1      イニスフリー (innisfree) ノーセバム ミネラルパウダー N 5グラム (x 1)       イニスフリー    825   \n",
       "2       メラノCC ディープクリア酵素洗顔 130g 酵素×ビタミンC配合 洗顔フォーム 毛穴ケア        メラノCC    715   \n",
       "3   アテニア (Attenir) スキンクリア クレンズオイル アロマタイプ [ レギュラーボト...         アテニア  1,870   \n",
       "4             ビオレUＶ アクアリッチ アクアプロテクトミスト 60ミリリットル (x 1)        ビオレUＶ    891   \n",
       "..                                                ...          ...    ...   \n",
       "95  ビオレ ザフェイス ディープモイスト つめかえ用 340ml(約2.1回分)【泡洗顔】【まさ...          ビオレ  1,078   \n",
       "96  【先行販売】ビオレ ザクレンズ オイルメイク落とし 本体 190ml クレンジングオイル ク...          ビオレ  1,192   \n",
       "97                  大洋 製薬 ワセリンHG クリーム 単品 100グラム (x 1)           大洋    784   \n",
       "98  【医薬部外品】ビオレ マシュマロホイップ 薬用アクネケア つめかえ用 大容量 洗顔 さわやか...          ビオレ  1,089   \n",
       "99               無印良品 マイルド保湿洗顔フォーム(大容量) 200g 37280724         無印良品    990   \n",
       "\n",
       "                                                  URL  ... アットコスメ  \\\n",
       "0   https://www.amazon.co.jp/VTCOSMETICS-%E3%83%96...  ...      1   \n",
       "1   https://www.amazon.co.jp/%E3%82%A4%E3%83%8B%E3...  ...      2   \n",
       "2   https://www.amazon.co.jp/%E3%80%90%E5%85%88%E8...  ...      3   \n",
       "3   https://www.amazon.co.jp/Attenir-%E3%82%B9%E3%...  ...      4   \n",
       "4   https://www.amazon.co.jp/%E3%83%93%E3%82%AA%E3...  ...      5   \n",
       "..                                                ...  ...    ...   \n",
       "95  https://www.amazon.co.jp/%E3%83%93%E3%82%AA%E3...  ...     96   \n",
       "96  https://www.amazon.co.jp/%E3%80%90%E5%85%88%E8...  ...     97   \n",
       "97  https://www.amazon.co.jp/%E5%A4%A7%E6%B4%8B%E8...  ...     98   \n",
       "98  https://www.amazon.co.jp/%E3%80%90%E5%8C%BB%E8...  ...     99   \n",
       "99  https://www.amazon.co.jp/%E7%84%A1%E5%8D%B0%E8...  ...    100   \n",
       "\n",
       "                        商品名           会社名      価格  \\\n",
       "0                  リップモンスター           ケイト     880   \n",
       "1       リポソーム アドバンスト リペアセラム       コスメデコルテ   8,250   \n",
       "2      リポソーム アドバンスト リペアクリーム       コスメデコルテ  11,000   \n",
       "3   ディオール アディクト リップ マキシマイザー         ディオール   4,620   \n",
       "4              クリーミータッチライナー        キャンメイク     715   \n",
       "..                      ...           ...     ...   \n",
       "95               ナチュラル チークN          セザンヌ     396   \n",
       "96                  パドル ブラシ   AVEDA(アヴェダ)   3,740   \n",
       "97           クリア サンケア スティック      SHISEIDO   3,080   \n",
       "98            マイラッシュ アドバンスト           オペラ     999   \n",
       "99              メイベリン スカイハイ  メイベリン ニューヨーク   1,639   \n",
       "\n",
       "                                                  URL レサージュ  \\\n",
       "0   https://www.cosme.net/product/product_id/10206...     1   \n",
       "1   https://www.cosme.net/product/product_id/10209...     2   \n",
       "2   https://www.cosme.net/product/product_id/10225...     3   \n",
       "3   https://www.cosme.net/product/product_id/10233...     4   \n",
       "4   https://www.cosme.net/product/product_id/10147...     5   \n",
       "..                                                ...   ...   \n",
       "95  https://www.cosme.net/product/product_id/27441...    96   \n",
       "96  https://www.cosme.net/product/product_id/34756...    97   \n",
       "97  https://www.cosme.net/product/product_id/10206...    98   \n",
       "98  https://www.cosme.net/product/product_id/10121...    99   \n",
       "99  https://www.cosme.net/product/product_id/10225...   100   \n",
       "\n",
       "                                                  商品名          会社名  \\\n",
       "0             ロート製薬 DRX 保湿乳液 ADパーフェクトバリア ボディミルク 130ml        ロート製薬   \n",
       "1                    ロート製薬 DRX ハイドロキノンクリーム HQダブルブライトE        ロート製薬   \n",
       "2             ロート製薬 DRX 保湿乳液 ADパーフェクトバリア フェイスミルク 50ml        ロート製薬   \n",
       "3                    ZO SKIN HEALTH ゼオスキンヘルス バランサートナー     ゼオスキンヘルス   \n",
       "4   NEW プラスリストアシリーズ plus RESTORE UVローション（日焼け止め）SPF...      プラスリストア   \n",
       "..                                                ...          ...   \n",
       "95  Revision Skincare リビジョン スキンケア D.E.J face cream...  リビジョン スキンケア   \n",
       "96                   ［デオドラント シリーズ］D-bar（ディーバー）制汗スティック        ディーバー   \n",
       "97  人気商品 お買い得 ［お買い得セット］HERRAS ミルキーピール エムディーソープ 乳酸菌...      ミルキーピール   \n",
       "98                           資生堂 ナビジョンDR『みずみずしく潤う』セット          資生堂   \n",
       "99                          資生堂 ナビジョンDR TAマイルドプロテクトUV          資生堂   \n",
       "\n",
       "                                                   価格  \\\n",
       "0                                               1,840   \n",
       "1                                               2,000   \n",
       "2                                               1,840   \n",
       "3                                               6,400   \n",
       "4                                               2,800   \n",
       "..                                                ...   \n",
       "95  <selenium.webdriver.remote.webelement.WebEleme...   \n",
       "96  <selenium.webdriver.remote.webelement.WebEleme...   \n",
       "97  <selenium.webdriver.remote.webelement.WebEleme...   \n",
       "98  <selenium.webdriver.remote.webelement.WebEleme...   \n",
       "99  <selenium.webdriver.remote.webelement.WebEleme...   \n",
       "\n",
       "                                                URL  \n",
       "0   https://www.lesage-store.jp/products/detail/417  \n",
       "1   https://www.lesage-store.jp/products/detail/411  \n",
       "2   https://www.lesage-store.jp/products/detail/418  \n",
       "3    https://www.lesage-store.jp/products/detail/78  \n",
       "4   https://www.lesage-store.jp/products/detail/303  \n",
       "..                                              ...  \n",
       "95  https://www.lesage-store.jp/products/detail/336  \n",
       "96   https://www.lesage-store.jp/products/detail/33  \n",
       "97  https://www.lesage-store.jp/products/detail/882  \n",
       "98  https://www.lesage-store.jp/products/detail/723  \n",
       "99  https://www.lesage-store.jp/products/detail/431  \n",
       "\n",
       "[100 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selenium関連\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "#データフレーム関連\n",
    "import pandas as pd\n",
    "\n",
    "#python関連\n",
    "import re\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "#Google Cloud Functions関連\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from google.cloud import storage\n",
    "\n",
    "#ログ出力関連\n",
    "import json\n",
    "import sys\n",
    "from logging import getLogger, StreamHandler, DEBUG, INFO, Formatter\n",
    "from logging import config\n",
    "\n",
    "#Google API認証関連\n",
    "import gspread\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient import discovery\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "#環境変数関連\n",
    "from env_local import (\n",
    "    FILE_NAME, \n",
    "    RAKUTEN_URL,\n",
    "    AMAZON_URL,\n",
    "    YAHOO_URL,\n",
    "    QOO10_URL,\n",
    "    COSME_URL,\n",
    "    LESAGE_URL,\n",
    "    DRIVER_WAIT_TIME,\n",
    "    SCROLL_PAUSE_TIME,\n",
    "    MAX_SCROLL_ATTEMPTS,\n",
    "    TIME_ZONE,\n",
    "    TEMP_DIR,\n",
    "    PROJECT_ID,\n",
    "    SECRET_ID,\n",
    "    VERSION_ID,\n",
    "    BUCKET_NAME,\n",
    "    ZIP_BLOB_NAME,\n",
    "    ZIP_FILE,\n",
    "    GOOGLE_DRIVE_FOLDER,\n",
    "    GOOGLE_SPREADSHEET_PROPERTY,\n",
    "    SCOPES\n",
    ")\n",
    "\n",
    "#カスタムロガーの設定\n",
    "with open('logging_config.json', 'r') as f:\n",
    "    logger_config = json.load(f)\n",
    "config.dictConfig(logger_config)\n",
    "logger = getLogger('main')\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "        logger.info(f'[source_blob_name]のダウンロードが完了しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'[source_blob_name]のダウンロード中にエラーが発生しました：[e]')\n",
    "        raise\n",
    "    \n",
    "def setup_chrome_driver(binary_file_name):\n",
    "    #optionの設定\n",
    "    chrome_options = ChromeOptions()\n",
    "    chrome_options.binary_location = binary_file_name\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(\"--remote-debugging-port=9222\")    \n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1280x1696')\n",
    "    chrome_options.add_argument('--hide-scrollbars')\n",
    "    chrome_options.add_argument('--enable-logging')\n",
    "    chrome_options.add_argument('--log-level=0')\n",
    "    chrome_options.add_argument('--v=99')\n",
    "    chrome_options.add_argument('--single-process')\n",
    "    chrome_options.add_argument('--ignore-certificate-errors')\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (X11; Linux x86_64; rv:93.0) Gecko/20100101 Firefox/93.0\"\n",
    "    )\n",
    "    chrome_options.add_experimental_option(\"prefs\", {\n",
    "    \"profile.managed_default_content_settings.images\": 2,  # 画像の無効化\n",
    "    \"profile.managed_default_content_settings.plugins\": 2,  # プラグインの無効化\n",
    "})\n",
    "    return chrome_options\n",
    "\n",
    "def get_driver():\n",
    "    try:\n",
    "        with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
    "            zip_ref.extractall(TEMP_DIR)\n",
    "            logger.info('chrome-headless.zipの解凍に成功しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'chrome-headless.zipの解凍に失敗しました:[e]')\n",
    "        raise\n",
    "    chrome_driver_path = os.path.join(TEMP_DIR,'chrome-headless', 'chromedriver')\n",
    "    chrome_binary_path = os.path.join(TEMP_DIR,'chrome-headless', 'headless-chromium')\n",
    "    logger.debug(f'ChromeDriverのパス: {chrome_driver_path}')\n",
    "    logger.debug(f'ChromeDriverのパス: {chrome_binary_path}')\n",
    "    # ChromeDriverのオプション設定\n",
    "    options = setup_chrome_driver(chrome_binary_path)\n",
    "    try:\n",
    "        os.chmod(chrome_driver_path, 0o777)\n",
    "        os.chmod(chrome_binary_path, 0o777)\n",
    "        logger.info('chrome-headless配下フォルダに実行権限を付与しました')\n",
    "    except Exception as e:\n",
    "        logger.error('chrome-headless配下フォルダに実行権限が付与出来ませんでした')   \n",
    "    try:\n",
    "        driver = webdriver.Chrome(chrome_driver_path, options=options)\n",
    "        logger.info('ChromeDriverを起動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'ChromeDriverの起動に失敗しました: [e]')\n",
    "        raise\n",
    "    #サーバー負荷対策\n",
    "    driver.implicitly_wait(DRIVER_WAIT_TIME)\n",
    "    return driver\n",
    "\n",
    "\n",
    "#-----楽天スクレイピング-----\n",
    "def get_scraping_rakuten(driver, root, result_dict, web_site):\n",
    "    #正規表現抽出\n",
    "    def _get_rank_num(text):\n",
    "        pattern = r'\\d+(?=位)'\n",
    "        rank_num = re.search(pattern, text).group()\n",
    "        return rank_num\n",
    "    \n",
    "    def _get_price(text):\n",
    "        pattern = r'(\\d+,?\\d*)'\n",
    "        price = re.search(pattern, text).group()\n",
    "        return price\n",
    "    \n",
    "    def _get_pagelink(page_links):\n",
    "        a_elements = driver.find_elements(By.CSS_SELECTOR, \".pager a\")\n",
    "        for i, a_element in enumerate(a_elements):\n",
    "            if i > 1:\n",
    "                break\n",
    "            link = a_element.get_attribute(\"href\")\n",
    "            page_links.append(link)\n",
    "        page_link = page_links[0]\n",
    "        logger.debug(f'{web_site}：ページリンクを取得しました')\n",
    "        return page_link\n",
    "    \n",
    "    def _get_product_info_up_to_3():\n",
    "        div_elements = driver.find_elements(By.CSS_SELECTOR,\".rnkRanking_top3box\")\n",
    "        for div_element in div_elements:\n",
    "            #順位取得\n",
    "            img_element = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_rankIcon img\")\n",
    "            alt_value = img_element.get_attribute('alt')\n",
    "            rank_num = _get_rank_num(alt_value)\n",
    "            #商品名/URL取得\n",
    "            a_element = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_itemName a\")\n",
    "            prod_name = a_element.text\n",
    "            prod_url = a_element.get_attribute('href')\n",
    "            #会社名取得\n",
    "            a_element = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_shop a\")\n",
    "            company_name = a_element.text\n",
    "            #価格取得\n",
    "            div_element_child = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_price\")\n",
    "            price_str = div_element_child.text\n",
    "            price = _get_price(price_str)\n",
    "            #結果格納\n",
    "            result_dict[web_site].append(rank_num)\n",
    "            result_dict['商品名'].append(prod_name)\n",
    "            result_dict['会社名'].append(company_name)\n",
    "            result_dict['価格'].append(price)\n",
    "            result_dict['URL'].append(prod_url)\n",
    "        logger.debug(f'{web_site}：1～3位までの順位/商品名/会社名/価格/URLを取得しました')\n",
    "        \n",
    "    def _get_product_info_after_4(num_page):\n",
    "        if num_page == 1:\n",
    "            div_elements = driver.find_elements(By.CSS_SELECTOR,\".rnkRanking_after4box\")\n",
    "            for div_element in div_elements:\n",
    "                #順位取得\n",
    "                div_element_child = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_dispRank\")\n",
    "                div_element_child_str = div_element_child.text\n",
    "                rank_num = _get_rank_num(div_element_child_str)\n",
    "                #商品名/URL取得\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_itemName a\")\n",
    "                prod_name = a_element.text\n",
    "                prod_url = a_element.get_attribute('href')\n",
    "                #会社名取得\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_shop a\")\n",
    "                company_name = a_element.text\n",
    "                #価格取得\n",
    "                div_element_child = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_price\")\n",
    "                price_str = div_element_child.text\n",
    "                price = _get_price(price_str)\n",
    "                #結果格納\n",
    "                result_dict[web_site].append(rank_num)\n",
    "                result_dict['商品名'].append(prod_name)\n",
    "                result_dict['会社名'].append(company_name)\n",
    "                result_dict['価格'].append(price)\n",
    "                result_dict['URL'].append(prod_url)\n",
    "        elif num_page == 2:\n",
    "            div_elements = driver.find_elements(By.CSS_SELECTOR,\".rnkRanking_after4box\")\n",
    "            for i, div_element in enumerate(div_elements, start=81):\n",
    "                if i > 100:\n",
    "                    break\n",
    "                #順位取得\n",
    "                if i == 100:\n",
    "                    div_element_child = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_dispRank_overHundred\")\n",
    "                else:\n",
    "                    div_element_child = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_dispRank\")\n",
    "                div_element_child_str = div_element_child.text\n",
    "                rank_num = _get_rank_num(div_element_child_str)\n",
    "                #商品名/URL取得\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_itemName a\")\n",
    "                prod_name = a_element.text\n",
    "                prod_url = a_element.get_attribute('href')\n",
    "                #会社名取得\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_shop a\")\n",
    "                company_name = a_element.text\n",
    "                #価格取得\n",
    "                div_element_child = div_element.find_element(By.CSS_SELECTOR,\".rnkRanking_price\")\n",
    "                price_str = div_element_child.text\n",
    "                price = _get_price(price_str)\n",
    "                #結果格納\n",
    "                result_dict[web_site].append(rank_num)\n",
    "                result_dict['商品名'].append(prod_name)\n",
    "                result_dict['会社名'].append(company_name)\n",
    "                result_dict['価格'].append(price)\n",
    "                result_dict['URL'].append(prod_url)\n",
    "        logger.debug(f'{web_site}：4位以降の順位/商品名/会社名/価格/URLを取得しました')\n",
    "        \n",
    "    #1ページ目へ遷移\n",
    "    page_links = []\n",
    "    num_page = 1\n",
    "    try:\n",
    "        driver.get(root)\n",
    "        logger.debug(f'{web_site}：トップページへ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：トップページへ移動出来ませんでした: [e]')\n",
    "    #1ページ目\n",
    "    logger.debug('------1ページ目------')\n",
    "    page_link = _get_pagelink(page_links)\n",
    "    _get_product_info_up_to_3()\n",
    "    _get_product_info_after_4(num_page)\n",
    "    #2ページ目へ遷移\n",
    "    num_page += 1\n",
    "    try:\n",
    "        driver.get(page_link)\n",
    "        logger.debug(f'{web_site}：{num_page}ページ目へ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：{num_page}ページ目へ移動出来ませんでした: [e]')\n",
    "        raise\n",
    "    logger.debug(f'------{num_page}ページ目------')\n",
    "    _get_product_info_after_4(num_page)\n",
    "   \n",
    "    #取得データ確認\n",
    "    logger.debug(f'------取得結果------')\n",
    "    logger.debug(f'{web_site}：\"{web_site}\" {len(result_dict[f\"{web_site}\"])}個')\n",
    "    logger.debug(f'{web_site}：\"商品名\" {len(result_dict[\"商品名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"会社名\" {len(result_dict[\"会社名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"価格\" {len(result_dict[\"価格\"])}個')\n",
    "    logger.debug(f'{web_site}：\"URL\" {len(result_dict[\"URL\"])}個')\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "#-----Amazonスクレイピング-----\n",
    "def get_scraping_amazon(driver, root, result_dict, web_site):\n",
    "    #正規表現抽出\n",
    "    def _get_company_name(text):\n",
    "        # 特定の文字列を除く (ex:フェイスマスク)\n",
    "        text = re.sub(r'フェイスマスク', '', text)\n",
    "        #【】もしくは()とその中の文字列を除く\n",
    "        text = re.sub(r'【[^】]*】|\\([^)]*\\)', '', text)\n",
    "        # 例外文字列(半角が会社名に含まれている)のみ最初に抽出 (ex:ラ ロッシュ ポゼ)\n",
    "        pattern =  r\"ラ\\s*ロッシュ\\s*ポゼ\"\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        #文字列の先頭から単語文字または空白文字が1回以上続く部分を抽出\n",
    "        # pattern =  r'^.*?(?=[【(])|^[\\w\\s]+'\n",
    "        pattern =  r'^[\\w\\s]+'\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            result = match.group(0).split()[0]\n",
    "            return result\n",
    "    \n",
    "    def _get_price(text):\n",
    "        pattern = r'(\\d+,?\\d*)'\n",
    "        price = re.search(pattern, text).group()\n",
    "        return price\n",
    "        \n",
    "    def _get_pagelink():\n",
    "        a_element = driver.find_element(By.CSS_SELECTOR, \"li.a-normal a\")\n",
    "        page_link = a_element.get_attribute(\"href\")\n",
    "        logger.debug(f'{web_site}：ページリンクを取得しました')\n",
    "        return page_link\n",
    "    \n",
    "    #スクロールしてページを更新\n",
    "    def _scroll_page(driver):\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        attempts = 0\n",
    "        while attempts < 50:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            attempts += 1\n",
    "    \n",
    "    def _get_product_info():\n",
    "        div_elements = driver.find_elements(By.ID,\"gridItemRoot\")\n",
    "        for div_element in div_elements:\n",
    "            #順位取得\n",
    "            span_element = div_element.find_element(By.CSS_SELECTOR,\".zg-bdg-text\")\n",
    "            span_element_str = span_element.text\n",
    "            rank_num = span_element_str[1:]\n",
    "            #商品名/会社名/URL取得\n",
    "            a_element = div_element.find_element(By.CSS_SELECTOR,\".a-link-normal:nth-of-type(2)\")\n",
    "            prod_url = a_element.get_attribute('href')\n",
    "            prod_name = a_element.text\n",
    "            company_name = _get_company_name(prod_name)\n",
    "            #価格取得\n",
    "            try:\n",
    "                span_element = div_element.find_element(By.CSS_SELECTOR,\"._cDEzb_p13n-sc-price_3mJ9Z\")\n",
    "                price_str = span_element.text\n",
    "                price = _get_price(price_str)\n",
    "            except NoSuchElementException:\n",
    "                try:\n",
    "                    span_element = div_element.find_element(By.CSS_SELECTOR,\".p13n-sc-price\")\n",
    "                    price_str = span_element.text\n",
    "                    price = _get_price(price_str)\n",
    "                except NoSuchElementException:\n",
    "                    price = \"ー\"\n",
    "            #結果格納\n",
    "            result_dict[web_site].append(rank_num)\n",
    "            result_dict['商品名'].append(prod_name)\n",
    "            result_dict['会社名'].append(company_name)\n",
    "            result_dict['価格'].append(price)\n",
    "            result_dict['URL'].append(prod_url)\n",
    "        logger.debug(f'{web_site}：順位/商品名/会社名/価格/URLを取得しました')         \n",
    "            \n",
    "    #1ページ目へ遷移\n",
    "    num_page = 1\n",
    "    try:\n",
    "        driver.get(root)\n",
    "        logger.debug(f'{web_site}：トップページへ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：トップページへ移動出来ませんでした: [e]')\n",
    "    #1ページ目\n",
    "    logger.debug('------1ページ目------')\n",
    "    _scroll_page(driver)\n",
    "    page_link = _get_pagelink()\n",
    "    _get_product_info()\n",
    "    #2ページ目へ遷移\n",
    "    num_page += 1\n",
    "    try:\n",
    "        driver.get(page_link)\n",
    "        logger.debug(f'{web_site}：{num_page}ページ目へ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：{num_page}ページ目へ移動出来ませんでした: [e]')\n",
    "        raise\n",
    "    logger.debug(f'------{num_page}ページ目------')\n",
    "    _scroll_page(driver)\n",
    "    _get_product_info()\n",
    "   \n",
    "    #取得データ確認\n",
    "    logger.debug(f'------取得結果------')\n",
    "    logger.debug(f'{web_site}：\"{web_site}\" {len(result_dict[f\"{web_site}\"])}個')\n",
    "    logger.debug(f'{web_site}：\"商品名\" {len(result_dict[\"商品名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"会社名\" {len(result_dict[\"会社名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"価格\" {len(result_dict[\"価格\"])}個')\n",
    "    logger.debug(f'{web_site}：\"URL\" {len(result_dict[\"URL\"])}個')\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "#-----yahooショッピングスクレイピング-----\n",
    "def get_scraping_yahoo(driver, root, result_dict, web_site):\n",
    "    def _get_pagelink(page_links):\n",
    "        a_elements = driver.find_elements(By.CSS_SELECTOR, \"ul.elPager a\")\n",
    "        for a_element in a_elements:\n",
    "            link = a_element.get_attribute(\"href\")\n",
    "            page_links.append(link)\n",
    "        logger.debug(f'{web_site}：ページリンクを取得しました')\n",
    "        return page_links\n",
    "    \n",
    "    def _get_product_info():\n",
    "        li_elements = driver.find_elements(By.CSS_SELECTOR,\"li.elItem.isReflect\")\n",
    "        for i, li_element in enumerate(li_elements, start=1):\n",
    "            if i > 20:\n",
    "                break\n",
    "            #順位取得\n",
    "            span_element = li_element.find_element(By.CSS_SELECTOR,\".num\")\n",
    "            rank_num = span_element.text\n",
    "            #商品名/URL取得\n",
    "            a_element = li_element.find_element(By.CSS_SELECTOR,\".elTitle a\")\n",
    "            prod_url = a_element.get_attribute('href')\n",
    "            prod_name = a_element.text\n",
    "            #会社名取得\n",
    "            a_element = li_element.find_element(By.CSS_SELECTOR,\".elStore.dcStoreIcon a\")\n",
    "            company_name = a_element.text\n",
    "            #価格取得\n",
    "            try:\n",
    "                price = li_element.find_element(By.CSS_SELECTOR,\".elPriceBody\").text\n",
    "            except NoSuchElementException:\n",
    "                price = \"ー\"\n",
    "            \n",
    "            #結果格納\n",
    "            result_dict[web_site].append(rank_num)\n",
    "            result_dict['商品名'].append(prod_name)\n",
    "            result_dict['会社名'].append(company_name)\n",
    "            result_dict['価格'].append(price)\n",
    "            result_dict['URL'].append(prod_url)\n",
    "        logger.debug(f'{web_site}：順位/商品名/会社名/価格/URLを取得しました')       \n",
    "    \n",
    "    #1ページ目へ遷移\n",
    "    page_links = []\n",
    "    num_page = 1\n",
    "    try:\n",
    "        driver.get(root)\n",
    "        logger.debug(f'{web_site}：トップページへ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：トップページへ移動出来ませんでした: [e]')\n",
    "    #1ページ目\n",
    "    logger.debug('------1ページ目------')\n",
    "    page_links = _get_pagelink(page_links)\n",
    "    _get_product_info()\n",
    "    #2ページ目以降へ遷移\n",
    "    for page_link in page_links:\n",
    "        num_page += 1\n",
    "        try:\n",
    "            driver.get(page_link)\n",
    "            logger.debug(f'{web_site}：{num_page}ページ目へ移動しました')\n",
    "        except Exception as e:\n",
    "            logger.error(f'{web_site}：{num_page}ページ目へ移動出来ませんでした: [e]')\n",
    "            raise\n",
    "        logger.debug(f'------{num_page}ページ目------')\n",
    "        _get_product_info()\n",
    "    \n",
    "    #取得データ確認\n",
    "    logger.debug(f'------取得結果------')\n",
    "    logger.debug(f'{web_site}：\"{web_site}\" {len(result_dict[f\"{web_site}\"])}個')\n",
    "    logger.debug(f'{web_site}：\"商品名\" {len(result_dict[\"商品名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"会社名\" {len(result_dict[\"会社名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"価格\" {len(result_dict[\"価格\"])}個')\n",
    "    logger.debug(f'{web_site}：\"URL\" {len(result_dict[\"URL\"])}個')\n",
    "    \n",
    "    return result_dict\n",
    "    \n",
    "\n",
    "#-----Qoo10スクレイピング-----\n",
    "def get_scraping_qoo10(driver, root, result_dict, web_site):\n",
    "    \n",
    "    def _get_price(text):\n",
    "        pattern = r'(\\d+,?\\d*)'\n",
    "        price = re.search(pattern, text).group()\n",
    "        return price\n",
    "    \n",
    "    def _get_product_info():\n",
    "        div_elements = driver.find_elements(By.CSS_SELECTOR,\"div.item\")\n",
    "        for i, div_element in enumerate(div_elements, start=1):\n",
    "            if i > 100:\n",
    "                break\n",
    "            #順位取得\n",
    "            span_element = div_element.find_element(By.CSS_SELECTOR,\".rank\")\n",
    "            rank_num = span_element.text\n",
    "            #商品名/URL取得\n",
    "            a_element = div_element.find_element(By.CSS_SELECTOR,\"a.tt\")\n",
    "            prod_url = a_element.get_attribute('href')\n",
    "            prod_name = a_element.text\n",
    "            #会社名取得\n",
    "            try:\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\"a.txt_brand\")\n",
    "                company_name = a_element.text\n",
    "            except NoSuchElementException:\n",
    "                company_name = \"ー\"\n",
    "            #価格取得\n",
    "            strong_element = div_element.find_element(By.CSS_SELECTOR,\".prc strong\")\n",
    "            price_str = strong_element.text\n",
    "            price = _get_price(price_str)\n",
    "            \n",
    "            #結果格納\n",
    "            result_dict[web_site].append(rank_num)\n",
    "            result_dict['商品名'].append(prod_name)\n",
    "            result_dict['会社名'].append(company_name)\n",
    "            result_dict['価格'].append(price)\n",
    "            result_dict['URL'].append(prod_url)\n",
    "        logger.debug(f'{web_site}：順位/商品名/会社名/価格/URLを取得しました') \n",
    "        \n",
    "    #1ページ目へ遷移\n",
    "    try:\n",
    "        driver.get(root)\n",
    "        logger.debug(f'{web_site}：トップページへ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：トップページへ移動出来ませんでした: [e]')\n",
    "    #1ページ目\n",
    "    logger.debug('------1ページ目------')\n",
    "    _get_product_info()\n",
    "        \n",
    "     #取得データ確認\n",
    "    logger.debug(f'------取得結果------')\n",
    "    logger.debug(f'{web_site}：\"{web_site}\" {len(result_dict[f\"{web_site}\"])}個')\n",
    "    logger.debug(f'{web_site}：\"商品名\" {len(result_dict[\"商品名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"会社名\" {len(result_dict[\"会社名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"価格\" {len(result_dict[\"価格\"])}個')\n",
    "    logger.debug(f'{web_site}：\"URL\" {len(result_dict[\"URL\"])}個')\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "#-----アットコスメスクレイピング-----\n",
    "def get_scraping_cosme(driver, root, result_dict, web_site):\n",
    "    #正規表現抽出\n",
    "    def _get_rank_num(text):\n",
    "        pattern = r'\\d+(?=位)'\n",
    "        rank_num = re.search(pattern, text).group()\n",
    "        return rank_num\n",
    "    \n",
    "    def _get_price(text):\n",
    "        pattern = r'(\\d{1,3}(?:,\\d{3})*)(?=円)'\n",
    "        try:\n",
    "            price = re.search(pattern, text).group(1)\n",
    "        except Exception:\n",
    "            price = \"ー\"\n",
    "        return price\n",
    "    \n",
    "    def _get_pagelink(page_links):\n",
    "        a_elements = driver.find_elements(By.CSS_SELECTOR, \".cmn-modules-paging li a\")\n",
    "        for i, a_element in enumerate(a_elements):\n",
    "            if i > 8:\n",
    "                break\n",
    "            link = a_element.get_attribute(\"href\")\n",
    "            page_links.append(link)\n",
    "        logger.debug(f'{web_site}：ページリンクを取得しました')\n",
    "        return page_links\n",
    "    \n",
    "    def _get_product_info(num_page):\n",
    "        if num_page == 1:\n",
    "            div_elements = driver.find_elements(By.CSS_SELECTOR,\"#list-item dl.clearfix\")\n",
    "            for i, div_element in enumerate(div_elements):\n",
    "                #順位取得\n",
    "                span_element = div_element.find_element(By.CSS_SELECTOR,\".rank-num\")\n",
    "                if i < 3:\n",
    "                    img_element = span_element.find_element(By.CSS_SELECTOR,\"img\")\n",
    "                    alt_value = img_element.get_attribute('alt')\n",
    "                    rank_num = _get_rank_num(alt_value)\n",
    "                else:\n",
    "                    span_element_child = span_element.find_element(By.CSS_SELECTOR,\".num\")\n",
    "                    rank_num = span_element_child.text\n",
    "                #商品名/URL取得\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\".item a\")\n",
    "                prod_name = a_element.text\n",
    "                prod_url = a_element.get_attribute('href')\n",
    "                #会社名取得\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\".brand a:nth-of-type(1)\")\n",
    "                company_name = a_element.text\n",
    "                #価格取得\n",
    "                p_element = div_element.find_element(By.CSS_SELECTOR,\".price\")\n",
    "                price_str = p_element.text\n",
    "                price = _get_price(price_str)\n",
    "                #結果格納\n",
    "                result_dict[web_site].append(rank_num)\n",
    "                result_dict['商品名'].append(prod_name)\n",
    "                result_dict['会社名'].append(company_name)\n",
    "                result_dict['価格'].append(price)\n",
    "                result_dict['URL'].append(prod_url)\n",
    "        else:\n",
    "            div_elements = driver.find_elements(By.CSS_SELECTOR,\"#list-item dl.clearfix\")\n",
    "            for div_element in div_elements:\n",
    "                #順位取得\n",
    "                span_element = div_element.find_element(By.CSS_SELECTOR,\".rank-num\")\n",
    "                span_element_child = span_element.find_element(By.CSS_SELECTOR,\".num\")\n",
    "                rank_num = span_element_child.text\n",
    "                #商品名/URL取得\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\".item a\")\n",
    "                prod_name = a_element.text\n",
    "                prod_url = a_element.get_attribute('href')\n",
    "                #会社名取得\n",
    "                a_element = div_element.find_element(By.CSS_SELECTOR,\".brand a:nth-of-type(1)\")\n",
    "                company_name = a_element.text\n",
    "                #価格取得\n",
    "                p_element = div_element.find_element(By.CSS_SELECTOR,\".price\")\n",
    "                price_str = p_element.text\n",
    "                price = _get_price(price_str)\n",
    "                #結果格納\n",
    "                result_dict[web_site].append(rank_num)\n",
    "                result_dict['商品名'].append(prod_name)\n",
    "                result_dict['会社名'].append(company_name)\n",
    "                result_dict['価格'].append(price)\n",
    "                result_dict['URL'].append(prod_url)\n",
    "        logger.debug(f'{web_site}：順位/商品名/会社名/価格/URLを取得しました') \n",
    "       \n",
    "    #1ページ目へ遷移\n",
    "    page_links = []\n",
    "    num_page = 1\n",
    "    try:\n",
    "        driver.get(root)\n",
    "        logger.debug(f'{web_site}：トップページへ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：トップページへ移動出来ませんでした: [e]')\n",
    "    #1ページ目\n",
    "    logger.debug('------1ページ目------')\n",
    "    page_links = _get_pagelink(page_links)\n",
    "    _get_product_info(num_page)\n",
    "    #2ページ目以降へ遷移\n",
    "    for page_link in page_links:\n",
    "        num_page += 1\n",
    "        try:\n",
    "            driver.get(page_link)\n",
    "            logger.debug(f'{web_site}：{num_page}ページ目へ移動しました')\n",
    "        except Exception as e:\n",
    "            logger.error(f'{web_site}：{num_page}ページ目へ移動出来ませんでした: [e]')\n",
    "            raise\n",
    "        logger.debug(f'------{num_page}ページ目------')\n",
    "        _get_product_info(num_page)\n",
    "    \n",
    "    #取得データ確認\n",
    "    logger.debug(f'------取得結果------')\n",
    "    logger.debug(f'{web_site}：\"{web_site}\" {len(result_dict[f\"{web_site}\"])}個')\n",
    "    logger.debug(f'{web_site}：\"商品名\" {len(result_dict[\"商品名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"会社名\" {len(result_dict[\"会社名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"価格\" {len(result_dict[\"価格\"])}個')\n",
    "    logger.debug(f'{web_site}：\"URL\" {len(result_dict[\"URL\"])}個')\n",
    "    \n",
    "    return result_dict\n",
    "        \n",
    "\n",
    "#-----レサージュ-----\n",
    "def get_scraping_lesage(driver, root, result_dict, web_site):    \n",
    "    #正規表現抽出\n",
    "    def _get_company_name(text):\n",
    "        # 特定の文字列を除く (ex:お買い得 )\n",
    "        text = re.sub(r'お買い得 ', '', text)\n",
    "        #【】・()・［］とその中の文字列を除く\n",
    "        text = re.sub(r'【[^】]*】|\\([^)]*\\)|［[^］]*］', '', text)\n",
    "        # 例外文字列のみ最初に抽出\n",
    "        patterns =  [\n",
    "            r\"Ogshi\",\n",
    "            r\"資生堂\",\n",
    "            r\"ロート製薬\",\n",
    "            r\"ラッシュアディクト\",\n",
    "            r\"プラスリストア\",\n",
    "            r\"ウルトラV\",\n",
    "            r\"Lov me Touch\",\n",
    "            r\"アプローラ\",\n",
    "            r\"リビジョン スキンケア\",\n",
    "            r\"スキン52\"\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                return match.group(0)\n",
    "        #文字列の先頭からひらがなorカタカナが1回以上続く部分を抽出\n",
    "        # pattern =  r'^.*?(?=[【(])|^[\\w\\s]+'\n",
    "        pattern = r\"(?!\\[.*?\\])[\\u3040-\\u30FF]+\"\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            result = match.group(0).split()[0]\n",
    "            return result\n",
    "        \n",
    "    def _get_pagelink(page_links):\n",
    "        a_elements = driver.find_elements(By.CSS_SELECTOR, \"ul.ec-pager a\")\n",
    "        for i, a_element in enumerate(a_elements):\n",
    "            if i > 1:\n",
    "                break\n",
    "            link = a_element.get_attribute(\"href\")\n",
    "            page_links.append(link)\n",
    "        page_link = page_links[-1]\n",
    "        logger.debug(f'{web_site}：ページリンクを取得しました')\n",
    "        return page_link\n",
    "    \n",
    "    def _get_product_info(num_page):\n",
    "        if num_page == 1:\n",
    "            a_elements = driver.find_elements(By.CSS_SELECTOR,\"li.ec-shelfGrid__item a\")\n",
    "            for i, a_element in enumerate(a_elements, start=1):\n",
    "                #※順位はiにて取得\n",
    "                #URL取得\n",
    "                href_value = a_element.get_attribute('href')\n",
    "                #商品名/会社名取得\n",
    "                p_element = a_element.find_element(By.CSS_SELECTOR,\"p:nth-of-type(2)\")\n",
    "                product_name = p_element.text\n",
    "                company_name = _get_company_name(product_name)\n",
    "                #価格取得\n",
    "                price = a_element.find_element(By.CSS_SELECTOR,\".c-price\").text\n",
    "                #結果格納\n",
    "                result_dict[web_site].append(i)\n",
    "                result_dict['商品名'].append(product_name)\n",
    "                result_dict['会社名'].append(company_name)\n",
    "                result_dict['価格'].append(price)\n",
    "                result_dict['URL'].append(href_value)    \n",
    "        elif num_page == 2:\n",
    "            a_elements = driver.find_elements(By.CSS_SELECTOR,\"li.ec-shelfGrid__item a\")\n",
    "            for i, a_element in enumerate(a_elements, start=61):\n",
    "                if i > 100:\n",
    "                    break\n",
    "                #※順位はiにて取得\n",
    "                #URL取得\n",
    "                href_value = a_element.get_attribute('href')\n",
    "                #商品名/会社名取得\n",
    "                p_element = a_element.find_element(By.CSS_SELECTOR,\"p:nth-of-type(2)\")\n",
    "                product_name = p_element.text\n",
    "                company_name = _get_company_name(product_name)\n",
    "                #価格取得\n",
    "                price = a_element.find_element(By.CSS_SELECTOR,\".c-price\")\n",
    "                #結果格納\n",
    "                result_dict[web_site].append(i)\n",
    "                result_dict['商品名'].append(product_name)\n",
    "                result_dict['会社名'].append(company_name)\n",
    "                result_dict['価格'].append(price)\n",
    "                result_dict['URL'].append(href_value)        \n",
    "        logger.debug(f'{web_site}：順位/商品名/会社名/価格/URLを取得しました。')\n",
    "    \n",
    "    #1ページ目へ遷移\n",
    "    page_links = []\n",
    "    num_page = 1\n",
    "    try:\n",
    "        driver.get(root)\n",
    "        logger.debug(f'{web_site}：トップページへ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：トップページへ移動出来ませんでした: [e]')\n",
    "    #1ページ目(1位～60位)\n",
    "    logger.debug('------1ページ目------')\n",
    "    page_link = _get_pagelink(page_links)\n",
    "    _get_product_info(num_page)\n",
    "    #2ページ目以降へ遷移\n",
    "    num_page += 1\n",
    "    try:\n",
    "        driver.get(page_link)\n",
    "        logger.debug(f'{web_site}：{num_page}ページ目へ移動しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'{web_site}：{num_page}ページ目へ移動出来ませんでした: [e]')\n",
    "        raise\n",
    "    logger.debug(f'------{num_page}ページ目------')\n",
    "    _get_product_info(num_page)\n",
    "    \n",
    "    #取得データ確認\n",
    "    logger.debug(f'------取得結果------')\n",
    "    logger.debug(f'{web_site}：\"{web_site}\" {len(result_dict[f\"{web_site}\"])}個')\n",
    "    logger.debug(f'{web_site}：\"商品名\" {len(result_dict[\"商品名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"会社名\" {len(result_dict[\"会社名\"])}個')\n",
    "    logger.debug(f'{web_site}：\"価格\" {len(result_dict[\"価格\"])}個')\n",
    "    logger.debug(f'{web_site}：\"URL\" {len(result_dict[\"URL\"])}個')\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def post_pd_to_sheet(df, execution_time):\n",
    "    # サービスアカウントキーを使って認証\n",
    "    SERVICE_ACCOUNT_FILE = '/workspace/extended-study-382401-8676076ee48a.json'\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    # gspreadのクライアントとGoogle Drive APIクライアントを設定\n",
    "    gc = gspread.authorize(credentials)\n",
    "    # Google Sheets API clientを作成\n",
    "    service = discovery.build('sheets', 'v4', credentials=credentials)\n",
    "    # Google Drive API clientを作成\n",
    "    drive_service = discovery.build('drive', 'v3', credentials=credentials)\n",
    "    # Google DriveフォルダIDを指定\n",
    "    folder_id = GOOGLE_DRIVE_FOLDER\n",
    "    page_token = None\n",
    "    items = []\n",
    "    \n",
    "    # フォルダ内のスプレッドシートを検索\n",
    "    try:\n",
    "        condition_list = [\n",
    "            'mimeType=\"application/vnd.google-apps.spreadsheet\"',\n",
    "            \"trashed = false\",\n",
    "            f\"'{folder_id}' in parents\" \n",
    "        ]\n",
    "        conditions = \" and \".join(condition_list)\n",
    "        while True:\n",
    "            results = drive_service.files().list(\n",
    "                q=conditions,\n",
    "                fields=\"nextPageToken, files(id, name)\"\n",
    "            ).execute()\n",
    "            items.extend(results.get(\"files\", []))\n",
    "            page_token = results.get(\"nextPageToken\", None)\n",
    "            if page_token is None:\n",
    "                break\n",
    "        logger.info('データ取得フォルダのファイル一覧を取得しました')\n",
    "    except Exception as e:\n",
    "        logger.error('データ取得フォルダのファイル一覧が取得出来ませんでした。')\n",
    "        raise\n",
    "    \n",
    "    #フォルダ内の各スプレッドシートにデータフレームを転記\n",
    "    is_updated = False\n",
    "    template_sheet_name = 'template'\n",
    "    new_sheet_name = f'{execution_time.hour}時取得'\n",
    "    for item in items:\n",
    "        #後続の日付が変わった場合のコピー元idを取得\n",
    "        if f'{FILE_NAME}' in item['name']:\n",
    "            spreadsheet_id = item['id']\n",
    "        if f'【{execution_time.strftime(\"%Y-%m-%d\")}】{FILE_NAME}' in item['name']:\n",
    "            spreadsheet_id = item['id']\n",
    "            spreadsheet = gc.open_by_key(spreadsheet_id)\n",
    "            template_sheet = spreadsheet.worksheet(template_sheet_name)\n",
    "            try:\n",
    "                new_sheet = spreadsheet.worksheet(new_sheet_name)\n",
    "                set_with_dataframe(new_sheet, df)\n",
    "                logger.info(f\"シート {new_sheet_name} はすでに存在しているため上書きしました。\")\n",
    "            except gspread.exceptions.WorksheetNotFound:\n",
    "                new_sheet = spreadsheet.duplicate_sheet(template_sheet.id, new_sheet_name=new_sheet_name)\n",
    "                set_with_dataframe(new_sheet, df)\n",
    "                logger.info(f\"シート {new_sheet_name} を新規作成しました。\")\n",
    "            is_updated = True\n",
    "            break\n",
    "    #日付が変わった場合の処理\n",
    "    if not is_updated:\n",
    "        new_file_name = f'【{execution_time.strftime(\"%Y-%m-%d\")}】{FILE_NAME}'\n",
    "        body = {\n",
    "            'name': new_file_name,\n",
    "            'parents':[folder_id]\n",
    "        }\n",
    "        try:\n",
    "            new_spreadsheet = drive_service.files().copy(fileId=spreadsheet_id, body=body).execute()\n",
    "            new_spreadsheet_id = new_spreadsheet.get('id')\n",
    "            logger.info(f'新しいファイル {new_file_name} を作成しました。')\n",
    "        except Exception as e:\n",
    "            logger.error(f'新しい日付ファイルが作成できませんでした。: {e}')\n",
    "            new_spreadsheet_id = None\n",
    "            raise\n",
    "        # 新しくコピーされた日付ファイルへ転記\n",
    "        if new_spreadsheet_id:\n",
    "            spreadsheet = gc.open_by_key(new_spreadsheet_id)\n",
    "        # templateと0時ファイル以外を削除\n",
    "        for other_sheet in spreadsheet.worksheets():\n",
    "            if other_sheet.title != 'template' and other_sheet.title != new_sheet_name:\n",
    "                spreadsheet.del_worksheet(other_sheet)\n",
    "        new_sheet = spreadsheet.worksheet(new_sheet_name)\n",
    "        set_with_dataframe(new_sheet, df)\n",
    "        \n",
    "    # 新シートの表示設定を非表示→表示 + 末尾へ移動\n",
    "    sheets_count = len(spreadsheet.worksheets())\n",
    "    GOOGLE_SPREADSHEET_PROPERTY['requests'][0]['updateSheetProperties']['properties']['sheetId'] = new_sheet.id\n",
    "    GOOGLE_SPREADSHEET_PROPERTY['requests'][1]['updateSheetProperties']['properties']['sheetId'] = new_sheet.id\n",
    "    GOOGLE_SPREADSHEET_PROPERTY['requests'][1]['updateSheetProperties']['properties']['index'] = sheets_count\n",
    "    body = GOOGLE_SPREADSHEET_PROPERTY\n",
    "    service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=body).execute()\n",
    "    logger.info('GoogleSpreadSheetへ転記しました')\n",
    "\n",
    "def main():\n",
    "    logger.info('------スクレイピングを開始します------')    \n",
    "    #実行時間取得\n",
    "    jst = pytz.timezone(TIME_ZONE)\n",
    "    execution_time = datetime.now(jst)\n",
    "    try:\n",
    "        driver = get_driver()\n",
    "        logger.info('ドライバ準備完了')\n",
    "    except Exception as e:\n",
    "        logger.error(f'ドライバ準備に失敗しました: [{e}]')\n",
    "        raise\n",
    "    \n",
    "    web_sites = [\"楽天\", \"Amazon\", \"ヤフーショッピング\", \"Qoo10\", \"アットコスメ\", \"レサージュ\"]\n",
    "    for web_site in web_sites:\n",
    "        result_dict = {\n",
    "            f'{web_site}': [], \n",
    "            '商品名': [], \n",
    "            '会社名': [],\n",
    "            '価格':[],\n",
    "            'URL': [],\n",
    "        }\n",
    "        if web_site == \"楽天\":\n",
    "            try:\n",
    "                logger.info('------楽天取得開始------')\n",
    "                result_dict_rakuten = get_scraping_rakuten(driver, RAKUTEN_URL, result_dict, web_site)\n",
    "                logger.info('------楽天取得完了------')\n",
    "            except Exception as e:\n",
    "                logger.error(f'楽天取得に失敗しました: [{e}]')\n",
    "                driver.quit()\n",
    "                raise\n",
    "        elif web_site == \"Amazon\":\n",
    "            try:\n",
    "                logger.info('------AMAZON取得開始------')\n",
    "                result_dict_amazon = get_scraping_amazon(driver, AMAZON_URL, result_dict, web_site)\n",
    "                logger.info('------AMAZON取得完了------')\n",
    "            except Exception as e:\n",
    "                logger.error(f'AMAZON取得に失敗しました: [{e}]')\n",
    "                driver.quit()\n",
    "                raise\n",
    "        elif web_site == \"ヤフーショッピング\":\n",
    "            try:\n",
    "                logger.info('------Yahoo取得開始------')\n",
    "                result_dict_yahoo = get_scraping_yahoo(driver, YAHOO_URL, result_dict, web_site)\n",
    "                logger.info('------Yahoo取得完了------')\n",
    "            except Exception as e:\n",
    "                logger.error(f'Yahoo取得に失敗しました: [{e}]')\n",
    "                driver.quit()\n",
    "                raise\n",
    "        elif web_site == \"Qoo10\":\n",
    "            try:\n",
    "                logger.info('------QOO10取得開始------')\n",
    "                result_dict_qoo10 = get_scraping_qoo10(driver, QOO10_URL, result_dict, web_site)\n",
    "                logger.info('------QOO10取得完了------')\n",
    "            except Exception as e:\n",
    "                logger.error(f'QOO10取得に失敗しました: [{e}]')\n",
    "                driver.quit()\n",
    "                raise\n",
    "        elif web_site == \"アットコスメ\":\n",
    "            try:\n",
    "                logger.info('------アットコスメ取得開始------')\n",
    "                result_dict_cosme = get_scraping_cosme(driver, COSME_URL, result_dict, web_site)\n",
    "                logger.info('------アットコスメ取得完了------')\n",
    "            except Exception as e:\n",
    "                logger.error(f'アットコスメ取得に失敗しました: [{e}]')\n",
    "                driver.quit()\n",
    "                raise\n",
    "        elif web_site == \"レサージュ\":\n",
    "            try:\n",
    "                logger.info('------レサージュ取得開始------')\n",
    "                result_dict_lesage = get_scraping_lesage(driver, LESAGE_URL, result_dict, web_site)\n",
    "                logger.info('------レサージュ取得完了------')\n",
    "            except Exception as e:\n",
    "                logger.error(f'レサージュ取得に失敗しました: [{e}]')\n",
    "                driver.quit()\n",
    "                raise\n",
    "    #ドライバ解放\n",
    "    driver.quit()\n",
    "    \n",
    "    #転記用データフレーム作成\n",
    "    result_df_rakuten = pd.DataFrame(result_dict_rakuten)\n",
    "    result_df_amazon = pd.DataFrame(result_dict_amazon)\n",
    "    result_df_yahoo = pd.DataFrame(result_dict_yahoo)\n",
    "    result_df_qoo10 = pd.DataFrame(result_dict_qoo10)\n",
    "    result_df_cosme = pd.DataFrame(result_dict_cosme)\n",
    "    result_df_lesage = pd.DataFrame(result_dict_lesage)\n",
    "    \n",
    "    # データフレーム結合\n",
    "    result_df = pd.concat([\n",
    "        result_df_rakuten,\n",
    "        result_df_amazon,\n",
    "        result_df_yahoo,\n",
    "        result_df_qoo10,\n",
    "        result_df_cosme,\n",
    "        result_df_lesage\n",
    "    ],axis=1)\n",
    "    \n",
    "    try:\n",
    "        post_pd_to_sheet(result_df, execution_time)\n",
    "        logger.info('GoogleSpreadSheetへの転記が完了しました')\n",
    "    except Exception as e:\n",
    "        logger.error(f'GoogleSpreadSheetへの転記に失敗しました:[{e}]')\n",
    "        raise\n",
    "    logger.info('------スクレイピングを終了します------')\n",
    "    \n",
    "    return result_df\n",
    "    \n",
    "result = main()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2caa2-85eb-443f-aa30-8f03e7bef0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97905d-bce9-4992-bb26-a5e535be359b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
